{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRYSu48huSUW",
        "outputId": "64e4984b-4b62-4709-847a-a021d4730a8f"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain openai tiktoken chromadb "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-KFB7J_u_3L",
        "outputId": "a29ed872-d0bc-41ef-bb72-b6a0fe426a08"
      },
      "outputs": [],
      "source": [
        "!pip show langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l6XPLPVrqEaV"
      },
      "outputs": [],
      "source": [
        "!wget -q https://www.dropbox.com/s/vs6ocyvpzzncvwh/new_articles.zip\n",
        "!unzip -q new_articles.zip -d new_articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AnZQpL_IZZZ"
      },
      "source": [
        "# LangChain multi-doc retriever with ChromaDB\n",
        "\n",
        "***New Points***\n",
        "- Multiple Files\n",
        "- ChromaDB\n",
        "- Source info \n",
        "- gpt-3.5-turbo API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqwsGJDhvAQ5"
      },
      "source": [
        "## Setting up LangChain \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dNA4TsHpu6OM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "#os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XHVE9uFb3Ajj"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UcQKUId3X2M"
      },
      "source": [
        "## Load multiple and process documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PRSeXXc_3Ypj"
      },
      "outputs": [],
      "source": [
        "# Load and process the text files\n",
        "# loader = TextLoader('single_text_file.txt')\n",
        "loader = DirectoryLoader('./new_articles/', glob=\"./*.txt\", loader_cls=TextLoader)\n",
        "\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3__nT0D4Fkmg"
      },
      "outputs": [],
      "source": [
        "#splitting the text into\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlU5AlqY4gwj",
        "outputId": "d78fb098-3161-42cd-8ce9-5f98f4ef91d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "233"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bg6-9jwU4ja_",
        "outputId": "02f45055-0e09-42ba-9ff3-0e7850f06cae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='In a statement, its CEO, Sarah Cardell, also said:\\n\\nAI has burst into the public consciousness over the past few months but has been on our radar for some time. It’s a technology developing at speed and has the potential to transform the way businesses compete as well as drive substantial economic growth. It’s crucial that the potential benefits of this transformative technology are readily accessible to UK businesses and consumers while people remain protected from issues like false or misleading information. Our goal is to help this new, rapidly scaling technology develop in ways that ensure open, competitive markets and effective consumer protection.\\n\\nSpecifically, the U.K. competition regulator said its initial review of AI foundational models will:\\n\\nexamine how the competitive markets for foundation models and their use could evolve\\n\\nexplore what opportunities and risks these scenarios could bring for competition and consumer protection', metadata={'source': 'new_articles/05-04-cma-generative-ai-review.txt'})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsYsIy8F4cdm"
      },
      "source": [
        "## create the DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_eTIZwf4Dk2",
        "outputId": "1cd293c4-716c-402d-d41b-045b6a264041"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mydocker/.conda/envs/langchain/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "# Embed and store the texts\n",
        "# Supplying a persist_directory will store the embeddings on disk\n",
        "persist_directory = 'db'\n",
        "\n",
        "## here we are using OpenAI embeddings but in future we will swap out to local embeddings\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=texts, \n",
        "                                 embedding=embedding,\n",
        "                                 persist_directory=persist_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uRfD_Te-47lb"
      },
      "outputs": [],
      "source": [
        "# persiste the db to disk\n",
        "vectordb.persist()\n",
        "vectordb = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-h1y_eAHmD-",
        "outputId": "4a6097fb-e30e-4fa2-ff4d-b972ce7f6154"
      },
      "outputs": [],
      "source": [
        "# Now we can load the persisted database from disk, and use it as normal. \n",
        "vectordb = Chroma(persist_directory=persist_directory, \n",
        "                  embedding_function=embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siLXR-XT0JoI"
      },
      "source": [
        "## Make a retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6ObunFU30Lxh"
      },
      "outputs": [],
      "source": [
        "retriever = vectordb.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cYA-H59u0Skn"
      },
      "outputs": [],
      "source": [
        "docs = retriever.get_relevant_documents(\"How much money did Pando raise?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0iAuh_B0ZjE",
        "outputId": "2ffd1da1-ff6c-4ea9-c361-7230dbd5bdc0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jVWgPJXs1yRq"
      },
      "outputs": [],
      "source": [
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "H4N0IhRM0hHL",
        "outputId": "f8058122-1c8a-4c5b-f046-14a90eed5a3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'similarity'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever.search_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jXL9u-u0prF",
        "outputId": "2e6dd94e-bede-4e05-c841-9c755ecfef2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'k': 2}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever.search_kwargs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ia-4OXa5IeP"
      },
      "source": [
        "## Make a chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MGx8XblM4shW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mydocker/.conda/envs/langchain/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "# create the chain to answer questions \n",
        "qa_chain = RetrievalQA.from_chain_type(llm=OpenAI(), \n",
        "                                  chain_type=\"stuff\", \n",
        "                                  retriever=retriever, \n",
        "                                  return_source_documents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LZEo26mw8e5k"
      },
      "outputs": [],
      "source": [
        "## Cite sources\n",
        "def process_llm_response(llm_response):\n",
        "    print(llm_response['result'])\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKfX4vX-5RFT",
        "outputId": "b3902fd2-86cc-4020-86a9-99883a996d20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Prometheus is impressive because it is able to detect and intervene if interactions are going down a dark path and can retract or withdraw messages that may be harmful or malicious. It also has the capability to span the entire solar system, making it a groundbreaking technology for interplanetary communication.\n",
            "\n",
            "\n",
            "Sources:\n",
            "new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt\n",
            "new_articles/05-05-vint-cerf-on-the-exhilarating-mix-of-thrill-and-hazard-at-the-frontiers-of-tech.txt\n"
          ]
        }
      ],
      "source": [
        "# full example\n",
        "#query = \"How much money did Pando raise?\"\n",
        "#query = \"What make Prometheus so impressed ?\"\n",
        "query = \"What the different between DeepFloyd IF and models such as Stable Diffusion and DALL-E 2 ?\"\n",
        "llm_response = qa_chain.invoke(query)\n",
        "process_llm_response(llm_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olRm73t3rNt2",
        "outputId": "20fb1b17-6562-421d-a60e-c67b97ca67d7"
      },
      "outputs": [],
      "source": [
        "# break it down\n",
        "query = \"What is the news about Pando?\"\n",
        "llm_response = qa_chain(query)\n",
        "# process_llm_response(llm_response)\n",
        "llm_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg-e6fh6rNwz",
        "outputId": "4b8d1e0e-b039-4e21-c233-a6c308cc5e25"
      },
      "outputs": [],
      "source": [
        "query = \"Who led the round in Pando?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuFf8D-rrN0I",
        "outputId": "19c63b88-33e2-4400-eede-f2678231eccb"
      },
      "outputs": [],
      "source": [
        "query = \"What did databricks acquire?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5KETxphrN3d",
        "outputId": "4f4a7dfb-0f5b-4b72-b678-6def5d056d20"
      },
      "outputs": [],
      "source": [
        "query = \"What is generative ai?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "692pHNkFrN5z",
        "outputId": "85124452-c208-4ec4-a35d-be28503ddc42"
      },
      "outputs": [],
      "source": [
        "query = \"Who is CMA?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPIhZWAR5n3X",
        "outputId": "68914c62-f8ed-4e22-d889-7991df441d53"
      },
      "outputs": [],
      "source": [
        "qa_chain.retriever.search_type , qa_chain.retriever.vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_lp0_796P_-",
        "outputId": "64c01726-6e78-4c12-e409-2fdc839f6611"
      },
      "outputs": [],
      "source": [
        "print(qa_chain.combine_documents_chain.llm_chain.prompt.template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSxVCnNi5h1-"
      },
      "source": [
        "## Deleteing the DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7xmepGJ2GAE",
        "outputId": "92b53c84-ef81-4000-db5a-2c2ec09db311"
      },
      "outputs": [],
      "source": [
        "!zip -r db.zip ./db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jl84qGQt5Wu5"
      },
      "outputs": [],
      "source": [
        "# To cleanup, you can delete the collection\n",
        "vectordb.delete_collection()\n",
        "vectordb.persist()\n",
        "\n",
        "# delete the directory\n",
        "!rm -rf db/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2r0ZIBPJp-K"
      },
      "source": [
        "## Starting again loading the db\n",
        "\n",
        "restart the runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pc7CM_mTQAt",
        "outputId": "f8e311fb-7d68-43af-ffd6-f66a9259766a"
      },
      "outputs": [],
      "source": [
        "!unzip db.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "us3F8ZKeRiz2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "#os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qK1nY4PkKYGo"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "396RyNbS4EXx",
        "outputId": "502d5c81-0823-4c00-89ca-7b4dd08bee26"
      },
      "outputs": [],
      "source": [
        "persist_directory = 'db'\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "vectordb2 = Chroma(persist_directory=persist_directory, \n",
        "                  embedding_function=embedding,\n",
        "                   )\n",
        "\n",
        "retriever = vectordb2.as_retriever(search_kwargs={\"k\": 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3vkSxxYKCZ9"
      },
      "outputs": [],
      "source": [
        "# Set up the turbo LLM\n",
        "turbo_llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name='gpt-3.5-turbo'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsR60NH5KCfj"
      },
      "outputs": [],
      "source": [
        "# create the chain to answer questions \n",
        "qa_chain = RetrievalQA.from_chain_type(llm=turbo_llm, \n",
        "                                  chain_type=\"stuff\", \n",
        "                                  retriever=retriever, \n",
        "                                  return_source_documents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWulTG0eKCfk"
      },
      "outputs": [],
      "source": [
        "## Cite sources\n",
        "def process_llm_response(llm_response):\n",
        "    print(llm_response['result'])\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDp-g2FtKCfk",
        "outputId": "766f131a-daaf-462f-842a-f7bd10a081fd"
      },
      "outputs": [],
      "source": [
        "# full example\n",
        "query = \"How much money did Pando raise?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fPl26c-TbWw"
      },
      "source": [
        "### Chat prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwyuhrpu5XqM",
        "outputId": "0f2c8060-4002-49ba-8869-6a9990c2c6d6"
      },
      "outputs": [],
      "source": [
        "print(qa_chain.combine_documents_chain.llm_chain.prompt.messages[0].prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcWXvSCHRvHO",
        "outputId": "d7a3acee-9ef1-4c08-b2a0-187f2cd90c8f"
      },
      "outputs": [],
      "source": [
        "print(qa_chain.combine_documents_chain.llm_chain.prompt.messages[1].prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "978QWCeJSRdu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
